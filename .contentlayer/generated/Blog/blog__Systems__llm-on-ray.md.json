{
  "title": "Introducing Intel LLM-on-Ray: A New Era for Large Language Models",
  "date": "2023-12-09T00:00:00.000Z",
  "tags": [
    "LLM",
    "Ray",
    "Intel",
    "distributed-training"
  ],
  "draft": false,
  "summary": "Pretrain, finetune and serve LLMs on Intel platforms with Ray distributed framework.",
  "authors": [
    "default"
  ],
  "body": {
    "raw": "\nWe are thrilled to announce the launch of the [Intel LLM-on-Ray project](https://github.com/intel/llm-on-ray), a groundbreaking initiative designed to revolutionize the way large language models (LLMs) are pretrained, finetuned, and served on Intel platforms using the Ray framework.\n\n## Empowering LLMs with Intel and Ray\n\nThe Intel LLM-on-Ray project brings together the scalability of Ray with the robustness of Intel's hardware, delivering an unparalleled environment for working with LLMs. Researchers and developers can now leverage this powerful combination to train and deploy some of the most advanced AI models with greater ease and efficiency.\n\n## Key Features\n\n- **Scalability**: Handle massive datasets and model sizes without compromising on performance.\n- **Flexibility**: Pretrain and finetune models with custom workflows tailored to specific needs.\n- **Efficiency**: Optimize resource utilization on Intel platforms, reducing operational costs.\n- **Ease of Use**: Simplified APIs and tools make it easier to get started and scale up.\n\n## Get Started\n\nThe project is open-source and ready for contributions. We encourage you to join us in this journey and help shape the future of LLMs. For more information, visit the [Intel LLM-on-Ray GitHub repository](https://github.com/intel/llm-on-ray).\n\n## Join the Community\n\nBecome part of a growing community of developers and researchers passionate about the potential of LLMs. Share your insights, collaborate on projects, and learn from the expertise of others in the field.\n",
    "code": "var Component=(()=>{var m=Object.create;var r=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var f=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),L=(t,e)=>{for(var a in e)r(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of f(e))!g.call(t,i)&&i!==a&&r(t,i,{get:()=>e[i],enumerable:!(o=u(e,i))||o.enumerable});return t};var w=(t,e,a)=>(a=t!=null?m(p(t)):{},s(e||!t||!t.__esModule?r(a,\"default\",{value:t,enumerable:!0}):a,t)),b=t=>s(r({},\"__esModule\",{value:!0}),t);var d=y((j,l)=>{l.exports=_jsx_runtime});var x={};L(x,{default:()=>h,frontmatter:()=>I});var n=w(d()),I={title:\"Introducing Intel LLM-on-Ray: A New Era for Large Language Models\",date:\"2023-12-09\",tags:[\"LLM\",\"Ray\",\"Intel\",\"distributed-training\"],draft:!1,summary:\"Pretrain, finetune and serve LLMs on Intel platforms with Ray distributed framework.\",authors:[\"default\"]};function c(t){let e={a:\"a\",h2:\"h2\",li:\"li\",p:\"p\",span:\"span\",strong:\"strong\",ul:\"ul\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"We are thrilled to announce the launch of the \",(0,n.jsx)(e.a,{href:\"https://github.com/intel/llm-on-ray\",children:\"Intel LLM-on-Ray project\"}),\", a groundbreaking initiative designed to revolutionize the way large language models (LLMs) are pretrained, finetuned, and served on Intel platforms using the Ray framework.\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"empowering-llms-with-intel-and-ray\",className:\"content-header\",children:[(0,n.jsx)(e.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#empowering-llms-with-intel-and-ray\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Empowering LLMs with Intel and Ray\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The Intel LLM-on-Ray project brings together the scalability of Ray with the robustness of Intel's hardware, delivering an unparalleled environment for working with LLMs. Researchers and developers can now leverage this powerful combination to train and deploy some of the most advanced AI models with greater ease and efficiency.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"key-features\",className:\"content-header\",children:[(0,n.jsx)(e.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#key-features\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Key Features\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Scalability\"}),\": Handle massive datasets and model sizes without compromising on performance.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Flexibility\"}),\": Pretrain and finetune models with custom workflows tailored to specific needs.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Efficiency\"}),\": Optimize resource utilization on Intel platforms, reducing operational costs.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Ease of Use\"}),\": Simplified APIs and tools make it easier to get started and scale up.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"get-started\",className:\"content-header\",children:[(0,n.jsx)(e.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#get-started\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Get Started\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The project is open-source and ready for contributions. We encourage you to join us in this journey and help shape the future of LLMs. For more information, visit the \",(0,n.jsx)(e.a,{href:\"https://github.com/intel/llm-on-ray\",children:\"Intel LLM-on-Ray GitHub repository\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"join-the-community\",className:\"content-header\",children:[(0,n.jsx)(e.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#join-the-community\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Join the Community\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Become part of a growing community of developers and researchers passionate about the potential of LLMs. Share your insights, collaborate on projects, and learn from the expertise of others in the field.\"})]})}function h(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(c,{...t})}):c(t)}return b(x);})();\n;return Component;"
  },
  "_id": "blog/Systems/llm-on-ray.md",
  "_raw": {
    "sourceFilePath": "blog/Systems/llm-on-ray.md",
    "sourceFileName": "llm-on-ray.md",
    "sourceFileDir": "blog/Systems",
    "contentType": "markdown",
    "flattenedPath": "blog/Systems/llm-on-ray"
  },
  "type": "Blog",
  "readingTime": {
    "text": "2 min read",
    "minutes": 1.1,
    "time": 66000,
    "words": 220
  },
  "slug": "llm-on-ray",
  "path": "blog/Systems/llm-on-ray",
  "toc": [
    {
      "value": "Empowering LLMs with Intel and Ray",
      "url": "#empowering-llms-with-intel-and-ray",
      "depth": 2
    },
    {
      "value": "Key Features",
      "url": "#key-features",
      "depth": 2
    },
    {
      "value": "Get Started",
      "url": "#get-started",
      "depth": 2
    },
    {
      "value": "Join the Community",
      "url": "#join-the-community",
      "depth": 2
    }
  ],
  "structuredData": {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Introducing Intel LLM-on-Ray: A New Era for Large Language Models",
    "datePublished": "2023-12-09T00:00:00.000Z",
    "dateModified": "2023-12-09T00:00:00.000Z",
    "description": "Pretrain, finetune and serve LLMs on Intel platforms with Ray distributed framework.",
    "url": "https://zhangjian94cn.github.io/blog/llm-on-ray"
  }
}