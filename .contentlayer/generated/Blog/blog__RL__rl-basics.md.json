{
  "title": "强化学习基础（1）",
  "date": "2023-03-14T00:00:00.000Z",
  "tags": [
    "强化学习",
    "深度学习",
    "DQN"
  ],
  "draft": false,
  "summary": "强化学习入门：介绍 Agent、Environment、Policy 等基本概念，以及 DQN、Policy Network 和 Actor-Critic 方法。",
  "authors": [
    "default"
  ],
  "body": {
    "raw": "\n## Introduction\n\n**智能体（Agent）**是一个能够感知**环境（Environment）**中的**状态（State）**并采取**动作（Action）**的实体。智能体的目标是在某些时间步中，以最大化的**总回报或奖励（Reward）**完成任务。为了实现这个目标，智能体需要利用之前的经验和当前的信息来选择最佳的动作，这个选择过程根据**策略（Policy）**来执行。\n\n- **Agent**：智能体，目标是获得最大化的回报（reward）\n- **Environment**：环境，包括所有的状态、动作、奖励和转移规则\n- **Policy**：策略，将当前状态映射到动作的概率分布\n- **Action**：动作，智能体在环境中执行的行为\n- **Reward**：奖励，表示智能体行为良好或不良的量化反馈值\n\n## Key Concepts\n\n### Reward Function\n\n回报函数定义为从时刻 t 开始的折扣累积奖励：\n\n`R_t = Σ γ^(i-t) × r_i`\n\n其中，R_t 表示在时间 t 的回报值，γ ∈ [0,1] 是折扣因子，表示对未来奖励的重视程度。\n\n### Action-Value Function\n\n动作价值函数用来衡量智能体在某个状态下执行某个动作后遵循某个策略所能获得的期望长期回报：\n\n`Q_π(s, a) = E[R_{t+1} + γ × Q(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]`\n\n最优动作价值函数：\n\n`Q*(s,a) = max_π Q_π(s,a)`\n\n### State-Value Function\n\n状态价值函数衡量智能体在某个状态下遵循某个策略后所能获得的期望长期回报：\n\n`V_π(s) = E[R_{t+1} + γ × V(S_{t+1}) | S_t = s]`\n\n## Deep Q-Network (Value-Based)\n\nDeep Q-Network 用神经网络来近似 Q*(s, a)。DQN 的目标是让神经网络输出的 Q 值尽可能接近 Q* 值，从而在每个状态下选择能够使长期回报最大化的动作。\n\n损失函数：\n\n`L(θ) = E[(r + γ × max_a' Q(s',a';θ⁻) - Q(s,a;θ))²]`\n\n其中：\n\n- θ 是神经网络的参数\n- θ⁻ 是目标网络的参数\n- D 是经验回放的缓存\n- γ 是折扣因子\n\n> DQN 损失函数的设计用到了 Temporal Difference Learning，根据当前状态和动作的价值函数估计和下一个状态的最优动作的价值函数估计来计算 TD 误差。\n\n## Policy Network (Policy-Based)\n\nPolicy Network 使用一个神经网络来直接输出给定状态下的动作概率分布。\n\n**优点**：\n\n- 可以处理连续动作空间和高维状态空间\n- 可以避免最大化误差的累积\n\n状态价值函数和状态-动作价值函数之间的关系为：\n\n`V_π(s) = Σ π_θ(a|s) × Q_π(s,a)`\n\n即在不同策略概率下 Q 的期望。在 Policy Network 中，我们最大化状态价值函数 V_π(s)。\n\n## Actor-Critic Methods\n\nActor-Critic Methods 结合了 Value-Based Methods 以及 Policy-Based Methods，不仅获得了两种方法的优点，同时也在一定程度上避免了：\n\n1. **Value-Based 的问题**：高偏差（High bias），不能直接得到动作值输出，难以用于连续动作空间\n2. **Policy-Based 的问题**：高方差（High Variance），训练不稳定，策略收敛困难\n\n它包括两个部分：\n\n- **Actor**：策略网络 π_θ(a|s)，输入状态，输出动作\n- **Critic**：价值网络，输入状态或状态-动作对，输出价值函数 V_w(s) 或 Q_w(s, a)\n\n## Conclusion\n\n在本文中，我们学习了 RL 基本流程与一些关键性概念，包括回报函数（Reward Function），状态价值函数（State-Value Function）以及动作价值函数（Action-Value Function）。同时，我们还介绍了 Value-Based Methods 以及 Policy-Based Methods，更进一步，Actor-Critic Methods 结合了两种方案，并获得了相对更优的效果。\n",
    "code": "var Component=(()=>{var u=Object.create;var t=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var _=Object.getPrototypeOf,N=Object.prototype.hasOwnProperty;var g=(i,n)=>()=>(n||i((n={exports:{}}).exports,n),n.exports),f=(i,n)=>{for(var c in n)t(i,c,{get:n[c],enumerable:!0})},d=(i,n,c,l)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let r of m(n))!N.call(i,r)&&r!==c&&t(i,r,{get:()=>n[r],enumerable:!(l=p(n,r))||l.enumerable});return i};var k=(i,n,c)=>(c=i!=null?u(_(i)):{},d(n||!i||!i.__esModule?t(c,\"default\",{value:i,enumerable:!0}):c,i)),w=i=>d(t({},\"__esModule\",{value:!0}),i);var o=g((b,a)=>{a.exports=_jsx_runtime});var y={};f(y,{default:()=>s,frontmatter:()=>x});var e=k(o()),x={title:\"\\u5F3A\\u5316\\u5B66\\u4E60\\u57FA\\u7840\\uFF081\\uFF09\",date:\"2023-03-14\",tags:[\"\\u5F3A\\u5316\\u5B66\\u4E60\",\"\\u6DF1\\u5EA6\\u5B66\\u4E60\",\"DQN\"],draft:!1,summary:\"\\u5F3A\\u5316\\u5B66\\u4E60\\u5165\\u95E8\\uFF1A\\u4ECB\\u7ECD Agent\\u3001Environment\\u3001Policy \\u7B49\\u57FA\\u672C\\u6982\\u5FF5\\uFF0C\\u4EE5\\u53CA DQN\\u3001Policy Network \\u548C Actor-Critic \\u65B9\\u6CD5\\u3002\",authors:[\"default\"]};function h(i){let n={a:\"a\",blockquote:\"blockquote\",code:\"code\",h2:\"h2\",h3:\"h3\",li:\"li\",ol:\"ol\",p:\"p\",span:\"span\",strong:\"strong\",ul:\"ul\",...i.components};return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsxs)(n.h2,{id:\"introduction\",className:\"content-header\",children:[(0,e.jsx)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#introduction\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Introduction\"]}),`\n`,(0,e.jsxs)(n.p,{children:[\"**\\u667A\\u80FD\\u4F53\\uFF08Agent\\uFF09\",(0,e.jsx)(n.strong,{children:\"\\u662F\\u4E00\\u4E2A\\u80FD\\u591F\\u611F\\u77E5\"}),\"\\u73AF\\u5883\\uFF08Environment\\uFF09\",(0,e.jsx)(n.strong,{children:\"\\u4E2D\\u7684\"}),\"\\u72B6\\u6001\\uFF08State\\uFF09\",(0,e.jsx)(n.strong,{children:\"\\u5E76\\u91C7\\u53D6\"}),\"\\u52A8\\u4F5C\\uFF08Action\\uFF09\",(0,e.jsx)(n.strong,{children:\"\\u7684\\u5B9E\\u4F53\\u3002\\u667A\\u80FD\\u4F53\\u7684\\u76EE\\u6807\\u662F\\u5728\\u67D0\\u4E9B\\u65F6\\u95F4\\u6B65\\u4E2D\\uFF0C\\u4EE5\\u6700\\u5927\\u5316\\u7684\"}),\"\\u603B\\u56DE\\u62A5\\u6216\\u5956\\u52B1\\uFF08Reward\\uFF09\",(0,e.jsx)(n.strong,{children:\"\\u5B8C\\u6210\\u4EFB\\u52A1\\u3002\\u4E3A\\u4E86\\u5B9E\\u73B0\\u8FD9\\u4E2A\\u76EE\\u6807\\uFF0C\\u667A\\u80FD\\u4F53\\u9700\\u8981\\u5229\\u7528\\u4E4B\\u524D\\u7684\\u7ECF\\u9A8C\\u548C\\u5F53\\u524D\\u7684\\u4FE1\\u606F\\u6765\\u9009\\u62E9\\u6700\\u4F73\\u7684\\u52A8\\u4F5C\\uFF0C\\u8FD9\\u4E2A\\u9009\\u62E9\\u8FC7\\u7A0B\\u6839\\u636E\"}),\"\\u7B56\\u7565\\uFF08Policy\\uFF09**\\u6765\\u6267\\u884C\\u3002\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Agent\"}),\"\\uFF1A\\u667A\\u80FD\\u4F53\\uFF0C\\u76EE\\u6807\\u662F\\u83B7\\u5F97\\u6700\\u5927\\u5316\\u7684\\u56DE\\u62A5\\uFF08reward\\uFF09\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Environment\"}),\"\\uFF1A\\u73AF\\u5883\\uFF0C\\u5305\\u62EC\\u6240\\u6709\\u7684\\u72B6\\u6001\\u3001\\u52A8\\u4F5C\\u3001\\u5956\\u52B1\\u548C\\u8F6C\\u79FB\\u89C4\\u5219\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Policy\"}),\"\\uFF1A\\u7B56\\u7565\\uFF0C\\u5C06\\u5F53\\u524D\\u72B6\\u6001\\u6620\\u5C04\\u5230\\u52A8\\u4F5C\\u7684\\u6982\\u7387\\u5206\\u5E03\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Action\"}),\"\\uFF1A\\u52A8\\u4F5C\\uFF0C\\u667A\\u80FD\\u4F53\\u5728\\u73AF\\u5883\\u4E2D\\u6267\\u884C\\u7684\\u884C\\u4E3A\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Reward\"}),\"\\uFF1A\\u5956\\u52B1\\uFF0C\\u8868\\u793A\\u667A\\u80FD\\u4F53\\u884C\\u4E3A\\u826F\\u597D\\u6216\\u4E0D\\u826F\\u7684\\u91CF\\u5316\\u53CD\\u9988\\u503C\"]}),`\n`]}),`\n`,(0,e.jsxs)(n.h2,{id:\"key-concepts\",className:\"content-header\",children:[(0,e.jsx)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#key-concepts\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Key Concepts\"]}),`\n`,(0,e.jsxs)(n.h3,{id:\"reward-function\",className:\"content-header\",children:[(0,e.jsx)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#reward-function\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Reward Function\"]}),`\n`,(0,e.jsx)(n.p,{children:\"\\u56DE\\u62A5\\u51FD\\u6570\\u5B9A\\u4E49\\u4E3A\\u4ECE\\u65F6\\u523B t \\u5F00\\u59CB\\u7684\\u6298\\u6263\\u7D2F\\u79EF\\u5956\\u52B1\\uFF1A\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"R_t = \\u03A3 \\u03B3^(i-t) \\xD7 r_i\"})}),`\n`,(0,e.jsx)(n.p,{children:\"\\u5176\\u4E2D\\uFF0CR_t \\u8868\\u793A\\u5728\\u65F6\\u95F4 t \\u7684\\u56DE\\u62A5\\u503C\\uFF0C\\u03B3 \\u2208 [0,1] \\u662F\\u6298\\u6263\\u56E0\\u5B50\\uFF0C\\u8868\\u793A\\u5BF9\\u672A\\u6765\\u5956\\u52B1\\u7684\\u91CD\\u89C6\\u7A0B\\u5EA6\\u3002\"}),`\n`,(0,e.jsxs)(n.h3,{id:\"action-value-function\",className:\"content-header\",children:[(0,e.jsx)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#action-value-function\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Action-Value Function\"]}),`\n`,(0,e.jsx)(n.p,{children:\"\\u52A8\\u4F5C\\u4EF7\\u503C\\u51FD\\u6570\\u7528\\u6765\\u8861\\u91CF\\u667A\\u80FD\\u4F53\\u5728\\u67D0\\u4E2A\\u72B6\\u6001\\u4E0B\\u6267\\u884C\\u67D0\\u4E2A\\u52A8\\u4F5C\\u540E\\u9075\\u5FAA\\u67D0\\u4E2A\\u7B56\\u7565\\u6240\\u80FD\\u83B7\\u5F97\\u7684\\u671F\\u671B\\u957F\\u671F\\u56DE\\u62A5\\uFF1A\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"Q_\\u03C0(s, a) = E[R_{t+1} + \\u03B3 \\xD7 Q(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]\"})}),`\n`,(0,e.jsx)(n.p,{children:\"\\u6700\\u4F18\\u52A8\\u4F5C\\u4EF7\\u503C\\u51FD\\u6570\\uFF1A\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"Q*(s,a) = max_\\u03C0 Q_\\u03C0(s,a)\"})}),`\n`,(0,e.jsxs)(n.h3,{id:\"state-value-function\",className:\"content-header\",children:[(0,e.jsx)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#state-value-function\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"State-Value Function\"]}),`\n`,(0,e.jsx)(n.p,{children:\"\\u72B6\\u6001\\u4EF7\\u503C\\u51FD\\u6570\\u8861\\u91CF\\u667A\\u80FD\\u4F53\\u5728\\u67D0\\u4E2A\\u72B6\\u6001\\u4E0B\\u9075\\u5FAA\\u67D0\\u4E2A\\u7B56\\u7565\\u540E\\u6240\\u80FD\\u83B7\\u5F97\\u7684\\u671F\\u671B\\u957F\\u671F\\u56DE\\u62A5\\uFF1A\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"V_\\u03C0(s) = E[R_{t+1} + \\u03B3 \\xD7 V(S_{t+1}) | S_t = s]\"})}),`\n`,(0,e.jsxs)(n.h2,{id:\"deep-q-network-value-based\",className:\"content-header\",children:[(0,e.jsx)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#deep-q-network-value-based\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Deep Q-Network (Value-Based)\"]}),`\n`,(0,e.jsx)(n.p,{children:\"Deep Q-Network \\u7528\\u795E\\u7ECF\\u7F51\\u7EDC\\u6765\\u8FD1\\u4F3C Q*(s, a)\\u3002DQN \\u7684\\u76EE\\u6807\\u662F\\u8BA9\\u795E\\u7ECF\\u7F51\\u7EDC\\u8F93\\u51FA\\u7684 Q \\u503C\\u5C3D\\u53EF\\u80FD\\u63A5\\u8FD1 Q* \\u503C\\uFF0C\\u4ECE\\u800C\\u5728\\u6BCF\\u4E2A\\u72B6\\u6001\\u4E0B\\u9009\\u62E9\\u80FD\\u591F\\u4F7F\\u957F\\u671F\\u56DE\\u62A5\\u6700\\u5927\\u5316\\u7684\\u52A8\\u4F5C\\u3002\"}),`\n`,(0,e.jsx)(n.p,{children:\"\\u635F\\u5931\\u51FD\\u6570\\uFF1A\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"L(\\u03B8) = E[(r + \\u03B3 \\xD7 max_a' Q(s',a';\\u03B8\\u207B) - Q(s,a;\\u03B8))\\xB2]\"})}),`\n`,(0,e.jsx)(n.p,{children:\"\\u5176\\u4E2D\\uFF1A\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"\\u03B8 \\u662F\\u795E\\u7ECF\\u7F51\\u7EDC\\u7684\\u53C2\\u6570\"}),`\n`,(0,e.jsx)(n.li,{children:\"\\u03B8\\u207B \\u662F\\u76EE\\u6807\\u7F51\\u7EDC\\u7684\\u53C2\\u6570\"}),`\n`,(0,e.jsx)(n.li,{children:\"D \\u662F\\u7ECF\\u9A8C\\u56DE\\u653E\\u7684\\u7F13\\u5B58\"}),`\n`,(0,e.jsx)(n.li,{children:\"\\u03B3 \\u662F\\u6298\\u6263\\u56E0\\u5B50\"}),`\n`]}),`\n`,(0,e.jsxs)(n.blockquote,{children:[`\n`,(0,e.jsx)(n.p,{children:\"DQN \\u635F\\u5931\\u51FD\\u6570\\u7684\\u8BBE\\u8BA1\\u7528\\u5230\\u4E86 Temporal Difference Learning\\uFF0C\\u6839\\u636E\\u5F53\\u524D\\u72B6\\u6001\\u548C\\u52A8\\u4F5C\\u7684\\u4EF7\\u503C\\u51FD\\u6570\\u4F30\\u8BA1\\u548C\\u4E0B\\u4E00\\u4E2A\\u72B6\\u6001\\u7684\\u6700\\u4F18\\u52A8\\u4F5C\\u7684\\u4EF7\\u503C\\u51FD\\u6570\\u4F30\\u8BA1\\u6765\\u8BA1\\u7B97 TD \\u8BEF\\u5DEE\\u3002\"}),`\n`]}),`\n`,(0,e.jsxs)(n.h2,{id:\"policy-network-policy-based\",className:\"content-header\",children:[(0,e.jsx)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#policy-network-policy-based\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Policy Network (Policy-Based)\"]}),`\n`,(0,e.jsx)(n.p,{children:\"Policy Network \\u4F7F\\u7528\\u4E00\\u4E2A\\u795E\\u7ECF\\u7F51\\u7EDC\\u6765\\u76F4\\u63A5\\u8F93\\u51FA\\u7ED9\\u5B9A\\u72B6\\u6001\\u4E0B\\u7684\\u52A8\\u4F5C\\u6982\\u7387\\u5206\\u5E03\\u3002\"}),`\n`,(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"\\u4F18\\u70B9\"}),\"\\uFF1A\"]}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsx)(n.li,{children:\"\\u53EF\\u4EE5\\u5904\\u7406\\u8FDE\\u7EED\\u52A8\\u4F5C\\u7A7A\\u95F4\\u548C\\u9AD8\\u7EF4\\u72B6\\u6001\\u7A7A\\u95F4\"}),`\n`,(0,e.jsx)(n.li,{children:\"\\u53EF\\u4EE5\\u907F\\u514D\\u6700\\u5927\\u5316\\u8BEF\\u5DEE\\u7684\\u7D2F\\u79EF\"}),`\n`]}),`\n`,(0,e.jsx)(n.p,{children:\"\\u72B6\\u6001\\u4EF7\\u503C\\u51FD\\u6570\\u548C\\u72B6\\u6001-\\u52A8\\u4F5C\\u4EF7\\u503C\\u51FD\\u6570\\u4E4B\\u95F4\\u7684\\u5173\\u7CFB\\u4E3A\\uFF1A\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"V_\\u03C0(s) = \\u03A3 \\u03C0_\\u03B8(a|s) \\xD7 Q_\\u03C0(s,a)\"})}),`\n`,(0,e.jsx)(n.p,{children:\"\\u5373\\u5728\\u4E0D\\u540C\\u7B56\\u7565\\u6982\\u7387\\u4E0B Q \\u7684\\u671F\\u671B\\u3002\\u5728 Policy Network \\u4E2D\\uFF0C\\u6211\\u4EEC\\u6700\\u5927\\u5316\\u72B6\\u6001\\u4EF7\\u503C\\u51FD\\u6570 V_\\u03C0(s)\\u3002\"}),`\n`,(0,e.jsxs)(n.h2,{id:\"actor-critic-methods\",className:\"content-header\",children:[(0,e.jsx)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#actor-critic-methods\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Actor-Critic Methods\"]}),`\n`,(0,e.jsx)(n.p,{children:\"Actor-Critic Methods \\u7ED3\\u5408\\u4E86 Value-Based Methods \\u4EE5\\u53CA Policy-Based Methods\\uFF0C\\u4E0D\\u4EC5\\u83B7\\u5F97\\u4E86\\u4E24\\u79CD\\u65B9\\u6CD5\\u7684\\u4F18\\u70B9\\uFF0C\\u540C\\u65F6\\u4E5F\\u5728\\u4E00\\u5B9A\\u7A0B\\u5EA6\\u4E0A\\u907F\\u514D\\u4E86\\uFF1A\"}),`\n`,(0,e.jsxs)(n.ol,{children:[`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Value-Based \\u7684\\u95EE\\u9898\"}),\"\\uFF1A\\u9AD8\\u504F\\u5DEE\\uFF08High bias\\uFF09\\uFF0C\\u4E0D\\u80FD\\u76F4\\u63A5\\u5F97\\u5230\\u52A8\\u4F5C\\u503C\\u8F93\\u51FA\\uFF0C\\u96BE\\u4EE5\\u7528\\u4E8E\\u8FDE\\u7EED\\u52A8\\u4F5C\\u7A7A\\u95F4\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Policy-Based \\u7684\\u95EE\\u9898\"}),\"\\uFF1A\\u9AD8\\u65B9\\u5DEE\\uFF08High Variance\\uFF09\\uFF0C\\u8BAD\\u7EC3\\u4E0D\\u7A33\\u5B9A\\uFF0C\\u7B56\\u7565\\u6536\\u655B\\u56F0\\u96BE\"]}),`\n`]}),`\n`,(0,e.jsx)(n.p,{children:\"\\u5B83\\u5305\\u62EC\\u4E24\\u4E2A\\u90E8\\u5206\\uFF1A\"}),`\n`,(0,e.jsxs)(n.ul,{children:[`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Actor\"}),\"\\uFF1A\\u7B56\\u7565\\u7F51\\u7EDC \\u03C0_\\u03B8(a|s)\\uFF0C\\u8F93\\u5165\\u72B6\\u6001\\uFF0C\\u8F93\\u51FA\\u52A8\\u4F5C\"]}),`\n`,(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Critic\"}),\"\\uFF1A\\u4EF7\\u503C\\u7F51\\u7EDC\\uFF0C\\u8F93\\u5165\\u72B6\\u6001\\u6216\\u72B6\\u6001-\\u52A8\\u4F5C\\u5BF9\\uFF0C\\u8F93\\u51FA\\u4EF7\\u503C\\u51FD\\u6570 V_w(s) \\u6216 Q_w(s, a)\"]}),`\n`]}),`\n`,(0,e.jsxs)(n.h2,{id:\"conclusion\",className:\"content-header\",children:[(0,e.jsx)(n.a,{\"aria-hidden\":\"true\",tabIndex:\"-1\",href:\"#conclusion\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,e.jsx)(n.p,{children:\"\\u5728\\u672C\\u6587\\u4E2D\\uFF0C\\u6211\\u4EEC\\u5B66\\u4E60\\u4E86 RL \\u57FA\\u672C\\u6D41\\u7A0B\\u4E0E\\u4E00\\u4E9B\\u5173\\u952E\\u6027\\u6982\\u5FF5\\uFF0C\\u5305\\u62EC\\u56DE\\u62A5\\u51FD\\u6570\\uFF08Reward Function\\uFF09\\uFF0C\\u72B6\\u6001\\u4EF7\\u503C\\u51FD\\u6570\\uFF08State-Value Function\\uFF09\\u4EE5\\u53CA\\u52A8\\u4F5C\\u4EF7\\u503C\\u51FD\\u6570\\uFF08Action-Value Function\\uFF09\\u3002\\u540C\\u65F6\\uFF0C\\u6211\\u4EEC\\u8FD8\\u4ECB\\u7ECD\\u4E86 Value-Based Methods \\u4EE5\\u53CA Policy-Based Methods\\uFF0C\\u66F4\\u8FDB\\u4E00\\u6B65\\uFF0CActor-Critic Methods \\u7ED3\\u5408\\u4E86\\u4E24\\u79CD\\u65B9\\u6848\\uFF0C\\u5E76\\u83B7\\u5F97\\u4E86\\u76F8\\u5BF9\\u66F4\\u4F18\\u7684\\u6548\\u679C\\u3002\"})]})}function s(i={}){let{wrapper:n}=i.components||{};return n?(0,e.jsx)(n,{...i,children:(0,e.jsx)(h,{...i})}):h(i)}return w(y);})();\n;return Component;"
  },
  "_id": "blog/RL/rl-basics.md",
  "_raw": {
    "sourceFilePath": "blog/RL/rl-basics.md",
    "sourceFileName": "rl-basics.md",
    "sourceFileDir": "blog/RL",
    "contentType": "markdown",
    "flattenedPath": "blog/RL/rl-basics"
  },
  "type": "Blog",
  "readingTime": {
    "text": "5 min read",
    "minutes": 4.445,
    "time": 266700,
    "words": 889
  },
  "slug": "rl-basics",
  "path": "blog/RL/rl-basics",
  "toc": [
    {
      "value": "Introduction",
      "url": "#introduction",
      "depth": 2
    },
    {
      "value": "Key Concepts",
      "url": "#key-concepts",
      "depth": 2
    },
    {
      "value": "Reward Function",
      "url": "#reward-function",
      "depth": 3
    },
    {
      "value": "Action-Value Function",
      "url": "#action-value-function",
      "depth": 3
    },
    {
      "value": "State-Value Function",
      "url": "#state-value-function",
      "depth": 3
    },
    {
      "value": "Deep Q-Network (Value-Based)",
      "url": "#deep-q-network-value-based",
      "depth": 2
    },
    {
      "value": "Policy Network (Policy-Based)",
      "url": "#policy-network-policy-based",
      "depth": 2
    },
    {
      "value": "Actor-Critic Methods",
      "url": "#actor-critic-methods",
      "depth": 2
    },
    {
      "value": "Conclusion",
      "url": "#conclusion",
      "depth": 2
    }
  ],
  "structuredData": {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "强化学习基础（1）",
    "datePublished": "2023-03-14T00:00:00.000Z",
    "dateModified": "2023-03-14T00:00:00.000Z",
    "description": "强化学习入门：介绍 Agent、Environment、Policy 等基本概念，以及 DQN、Policy Network 和 Actor-Critic 方法。",
    "url": "https://zhangjian94cn.github.io/blog/rl-basics"
  }
}